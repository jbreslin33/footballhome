#!/bin/bash

# Football Home - Development Script
# Unified workflow for scraping, building, and running the application
#
# Usage:
#   ./dev.sh                    # Full rebuild (scrape + clean rebuild + start)
#   ./dev.sh --quick            # Quick restart (no scrape, keep DB volumes)
#   ./dev.sh --scrape-only      # Only scrape APSL data, don't rebuild
#   ./dev.sh --no-scrape        # Full rebuild but skip scraping (use existing data)
#   ./dev.sh --help             # Show this help

set -e

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m'

# Flags
QUICK_MODE=false
SCRAPE_ONLY=false
SKIP_SCRAPE=false

# Parse arguments
for arg in "$@"; do
    case $arg in
        --quick)
            QUICK_MODE=true
            SKIP_SCRAPE=true
            ;;
        --scrape-only)
            SCRAPE_ONLY=true
            ;;
        --no-scrape)
            SKIP_SCRAPE=true
            ;;
        --help|-h)
            echo "Football Home Development Script"
            echo ""
            echo "Usage:"
            echo "  ./dev.sh                Full rebuild (scrape + clean rebuild)"
            echo "  ./dev.sh --quick        Quick restart (no scrape, keep volumes)"
            echo "  ./dev.sh --scrape-only  Only scrape APSL data"
            echo "  ./dev.sh --no-scrape    Rebuild without scraping"
            echo "  ./dev.sh --help         Show this help"
            echo ""
            echo "Common workflows:"
            echo "  - Fresh setup:           ./dev.sh"
            echo "  - Code changes only:     ./dev.sh --quick"
            echo "  - Update APSL data:      ./dev.sh --scrape-only"
            echo "  - After git pull:        ./dev.sh --no-scrape"
            exit 0
            ;;
        *)
            echo -e "${RED}Unknown option: $arg${NC}"
            echo "Use --help for usage information"
            exit 1
            ;;
    esac
done

echo -e "${BLUE}========================================${NC}"
echo -e "${BLUE}Football Home - Development Workflow${NC}"
echo -e "${BLUE}========================================${NC}"
echo ""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# STEP 1: SCRAPE APSL DATA
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

if [ "$SKIP_SCRAPE" = false ]; then
    echo -e "${YELLOW}ğŸ“Š Step 1: Scraping APSL Data${NC}"
    echo -e "  This will take 5-15 minutes..."
    echo ""
    
    # Clean up old .copy.sql files before scraping (scraper will generate fresh ones)
    OLD_COPY_COUNT=$(ls database/data/*.copy.sql 2>/dev/null | wc -l)
    if [ "$OLD_COPY_COUNT" -gt 0 ]; then
        echo -e "${BLUE}ğŸ—‘ï¸  Cleaning up $OLD_COPY_COUNT old .copy.sql files...${NC}"
        rm database/data/*.copy.sql
    fi
    
    if node database/scripts/apsl-scraper/scrape-apsl.js; then
        echo ""
        echo -e "${GREEN}âœ“ APSL data scraped successfully${NC}"
    else
        echo -e "${RED}âœ— APSL scraping failed${NC}"
        exit 1
    fi
    
    echo ""
    echo -e "${BLUE}ğŸ“‹ Changes:${NC}"
    git status --short database/data/ || true
else
    echo -e "${BLUE}â­ï¸  Step 1: Skipping APSL scrape${NC}"
fi

# Exit if scrape-only mode
if [ "$SCRAPE_ONLY" = true ]; then
    echo ""
    echo -e "${GREEN}âœ“ Scrape complete! (scrape-only mode)${NC}"
    echo ""
    echo "Review changes with:"
    echo "  git status"
    echo "  git diff database/data/"
    exit 0
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# STEP 1.5: CONVERT SQL TO COPY FORMAT (for 20-40x faster loading)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# STEP 1.5: CONVERT SQL TO COPY FORMAT (for 20-40x faster loading)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# NOTE: Scraper now generates .copy.sql files directly, so we don't need to convert
# Just verify they exist
COPY_COUNT=$(ls database/data/*.copy.sql 2>/dev/null | wc -l)
if [ "$COPY_COUNT" -gt 0 ]; then
    echo ""
    echo -e "${GREEN}âœ“ Found $COPY_COUNT .copy.sql files (generated by scraper)${NC}"
else
    echo ""
    echo -e "${YELLOW}âš  No .copy.sql files found - database will load slower${NC}"
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# STEP 2: DOCKER BUILD & DEPLOY
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

echo ""
if [ "$QUICK_MODE" = true ]; then
    echo -e "${YELLOW}ğŸ”¨ Step 2: Quick Restart${NC}"
    echo -e "  - Keeping volumes (preserving database)"
    echo -e "  - Rebuilding images"
    echo ""
    echo -e "${YELLOW}ğŸ›‘ Stopping containers...${NC}"
    docker compose down
    echo -e "${GREEN}âœ“ Containers stopped${NC}"
    echo ""
    echo -e "${YELLOW}ğŸ”¨ Building images...${NC}"
    docker compose build --no-cache --progress=plain
    echo -e "${GREEN}âœ“ Images built${NC}"
else
    echo -e "${YELLOW}ğŸ”¨ Step 2: Full Rebuild${NC}"
    echo -e "  - Removing all volumes (fresh database)"
    echo -e "  - Clearing all caches"
    echo -e "  - Rebuilding all images"
    echo ""
    echo -e "${YELLOW}ğŸ›‘ Stopping containers and removing volumes...${NC}"
    docker compose down -v
    echo -e "${GREEN}âœ“ Containers stopped and volumes removed${NC}"
    echo ""
    echo -e "${YELLOW}ğŸ—‘ï¸  Clearing Docker build cache...${NC}"
    docker builder prune -f > /dev/null 2>&1
    echo -e "${GREEN}âœ“ Build cache cleared${NC}"
    echo ""
    echo -e "${YELLOW}ğŸ”¨ Building images from scratch...${NC}"
    docker compose build --no-cache --progress=plain
    echo -e "${GREEN}âœ“ Images built${NC}"
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# STEP 3: START CONTAINERS
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

echo ""
echo -e "${YELLOW}ğŸš€ Step 3: Starting containers...${NC}"
docker compose up -d

echo -e "${GREEN}âœ“ Startup complete!${NC}"
echo ""
echo -e "${BLUE}Services:${NC}"
echo "  Frontend:  http://localhost:3000"
echo "  Backend:   http://localhost:3001"
echo "  Database:  localhost:5432"
echo "  pgAdmin:   http://localhost:5050"
echo ""

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# STEP 4: WAIT FOR SERVICES TO BE READY
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

echo -e "${BLUE}Testing connectivity...${NC}"
echo -e "  Backend:  Waiting for health check..."
echo -e "            (Backend is waiting for database to initialize - showing SQL activity)"
echo -e "  ${BLUE}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"

BACKEND_READY=false
i=0

# Start showing database logs in background while waiting
LAST_QUERY=""
LAST_TABLE=""
ROW_COUNT=0
# Use --follow to show ALL logs (existing + new) not just new ones from this point forward
# This captures initialization that starts immediately when container launches
(docker logs --follow footballhome_db 2>&1 | while IFS= read -r line; do
    # Capture the full query being executed
    if echo "$line" | grep -qE "statement:"; then
        FULL_STATEMENT=$(echo "$line" | sed 's/^.*statement: //')
        LAST_QUERY=$(echo "$FULL_STATEMENT" | head -c 150)
        
        # Show all statements in real-time for debugging
        if echo "$FULL_STATEMENT" | grep -qE "(CREATE TABLE|ALTER TABLE|CREATE INDEX|COPY)"; then
            STMT_TYPE=$(echo "$FULL_STATEMENT" | grep -oE "^(CREATE TABLE|ALTER TABLE|CREATE INDEX|COPY)" | head -1)
            echo -e "\n  ${BLUE}â”‚ ğŸ”§ $STMT_TYPE...${NC}"
        fi
    fi
    
    # Track what table we're working on
    if echo "$line" | grep -q "INSERT INTO"; then
        TABLE=$(echo "$line" | grep -oP "INSERT INTO \K[^ (]+")
        if [ "$TABLE" != "$LAST_TABLE" ]; then
            echo -e "\n  ${YELLOW}â”‚ â• Inserting into: $TABLE${NC}"
            LAST_TABLE="$TABLE"
            ROW_COUNT=0
        fi
        ROW_COUNT=$((ROW_COUNT + 1))
        # Show progress every 10 rows
        if [ $((ROW_COUNT % 10)) -eq 0 ]; then
            printf "\r  ${YELLOW}â”‚${NC}    â†³ $ROW_COUNT rows inserted...  "
        fi
    elif echo "$line" | grep -qE "statement:.*CREATE TABLE"; then
        # Extract table name from statement line
        TABLE=$(echo "$line" | grep -oP "CREATE TABLE (?:IF (?:NOT )?EXISTS )?\K[^ ;(]+")
        if [ -z "$TABLE" ]; then
            # Fallback: look for word after TABLE
            TABLE=$(echo "$line" | sed -n 's/.*CREATE TABLE[^a-zA-Z_]*\([a-zA-Z_][a-zA-Z0-9_]*\).*/\1/p')
        fi
        if [ -n "$TABLE" ]; then
            echo -e "\n  ${GREEN}â”‚ âœ¨ Creating table: $TABLE${NC}"
            LAST_TABLE="$TABLE"
        fi
    elif echo "$line" | grep -qE "statement:.*COPY .* FROM"; then
        TABLE=$(echo "$line" | grep -oP "statement:.*COPY \K[^ (]+")
        COLUMNS=$(echo "$line" | grep -oP '\(\K[^)]+' | tr ',' '\n' | wc -l)
        echo -e "\n  ${BLUE}â”‚ ğŸ“¥ COPY $COLUMNS columns into: $TABLE${NC}"
        echo -e "  ${BLUE}â”‚${NC}    â†³ Reading bulk data..."
        LAST_TABLE="$TABLE"
    elif echo "$line" | grep -qE "^COPY [0-9]+"; then
        ROWS=$(echo "$line" | grep -oP "^COPY \K[0-9]+")
        echo -e "\n  ${GREEN}â”‚ âœ… Loaded $ROWS rows into $LAST_TABLE${NC}"
    elif echo "$line" | grep -qE "duration: [0-9]+\.[0-9]+ ms"; then
        DUR=$(echo "$line" | grep -oP "duration: \K[0-9.]+")
        # Show slow queries (>500ms) with details
        if (( $(echo "$DUR > 500" | bc -l) )); then
            if [ -n "$LAST_QUERY" ]; then
                echo -e "\n  ${YELLOW}â”‚ â±ï¸  SLOW (${DUR}ms): ${LAST_QUERY:0:80}...${NC}"
            else
                echo -e "\n  ${YELLOW}â”‚ â±ï¸  SLOW QUERY: ${DUR}ms${NC}"
            fi
        elif (( $(echo "$DUR > 100" | bc -l) )); then
            # Show medium queries (100-500ms) more briefly
            echo -e "\n  ${YELLOW}â”‚ â±ï¸  ${DUR}ms${NC}"
        fi
    fi
done) &
LOG_PID=$!

# Also show a heartbeat so user knows the script is still running
# Use a file to track elapsed time since subprocess can't access parent vars
WAIT_START=$(date +%s)
HEARTBEAT_PID=""
(while true; do
    sleep 5
    # If backend isn't ready yet, show we're still waiting
    if ! curl -s http://localhost:3001/health > /dev/null 2>&1; then
        ELAPSED=$(($(date +%s) - WAIT_START))
        printf "\r  ${BLUE}â”‚${NC} â³ Still initializing... (${ELAPSED}s elapsed)  "
    fi
done) &
HEARTBEAT_PID=$!

# Poll backend health with counter
while true; do
    i=$((i + 1))
    if curl -s http://localhost:3001/health > /dev/null 2>&1; then
        kill $LOG_PID 2>/dev/null || true
        kill $HEARTBEAT_PID 2>/dev/null || true
        wait $LOG_PID 2>/dev/null || true
        wait $HEARTBEAT_PID 2>/dev/null || true
        echo -e "\n  ${BLUE}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
        echo -e "  Backend:  ${GREEN}âœ“ Responding (took ${i}s)${NC}"
        BACKEND_READY=true
        break
    fi
    sleep 1
    
    # Timeout after 5 minutes
    if [ $i -ge 300 ]; then
        kill $LOG_PID 2>/dev/null || true
        kill $HEARTBEAT_PID 2>/dev/null || true
        wait $LOG_PID 2>/dev/null || true
        wait $HEARTBEAT_PID 2>/dev/null || true
        echo -e "\n  ${BLUE}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
        echo -e "  ${RED}âœ— Timeout waiting for backend${NC}"
        echo ""
        echo "Check logs with:"
        echo "  docker logs footballhome_backend"
        echo "  docker logs footballhome_db"
        exit 1
    fi
done

echo ""

# Test frontend
echo -e "  Frontend: Checking..."
if curl -s http://localhost:3000 > /dev/null 2>&1; then
    echo -e "  Frontend: ${GREEN}âœ“ Responding${NC}"
else
    echo -e "  Frontend: ${RED}âœ— Not responding${NC}"
fi

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# FINAL STATUS
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

echo ""
echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}âœ“ All services ready!${NC}"
echo -e "${GREEN}========================================${NC}"
echo ""
echo -e "${BLUE}Next Steps:${NC}"
echo "  1. Test login at http://localhost:3000"
echo "     Email: jbreslin@footballhome.org"
echo "     Password: 1893Soccer!"
echo ""
echo "  2. View logs:"
echo "     docker logs -f footballhome_backend"
echo "     docker logs -f footballhome_db"
echo ""

if [ "$SKIP_SCRAPE" = false ] && [ "$QUICK_MODE" = false ]; then
    echo "  3. If data updated, commit changes:"
    echo "     git add database/data/"
    echo "     git commit -m \"Update APSL data\""
    echo "     git push"
    echo ""
fi
