# Data Loading Strategy

## Overview
Football Home uses a hybrid approach for data management:
- **Static/Semi-Static Data**: Loaded from SQL files on database initialization (rebuild)
- **Dynamic Data**: Scraped regularly via `update.sh` and persists across rebuilds
- **One-Time Scraped Data**: Scraped once to generate SQL files, then committed to git

## Data Categories

### 1. Static Reference Data (SQL Files)
**Location**: `database/data/*.sql`  
**When Loaded**: Every rebuild via `docker-entrypoint-initdb.d`  
**Source**: Manually created by developers

**Files**:
- `00-schema.sql` - Schema + lookup tables (match_types, positions, source_systems, etc.)
- `013d-scrape-targets.sql` - Scrape target configurations
- `013e-scrape-actions.sql` - Scrape action types
- `013f-scrape-statuses.sql` - Scrape status types
- `013g-scrape-targets-state-machine.sql` - Scrape state machine
- `013h-scrape-target-junctions.sql` - Junction table definitions
- `014-continents.sql` - Continents reference data
- `020-persons.sql` - Manual person records
- `021-users.sql` - User accounts
- `022-user-emails.sql` - User email addresses
- `023-user-phones.sql` - User phone numbers
- `024-admins.sql` - Admin role assignments

**Philosophy**:
- Lookup tables that rarely change
- User/admin data created manually via admin dashboard
- Configuration data (scrape targets)
- Reference data (continents, countries, positions)

### 2. League Structure (SQL Files - NEEDS TO BE CREATED)
**Location**: `database/data/*.sql` (to be created)  
**When Loaded**: Every rebuild  
**Source**: Manually defined based on league knowledge

**Missing Files** (should be created):
- `030-organizations.sql` - APSL, CSL, CASA organizations
- `031-leagues.sql` - League definitions (APSL, CSL, CASA)
- `032-seasons.sql` - Known seasons for each league
- `033-conferences.sql` - Conference definitions (APSL: Metropolitan, National, etc.)
- `034-divisions.sql` - Division definitions (CSL: Division 1, Division 2, etc.)
- `035-clubs.sql` - Manually managed clubs (Lighthouse 1893 SC, etc.)

**Why These Should Be in SQL**:
- League structures are known in advance
- Prevents duplicate divisions/seasons when scrapers run multiple times
- Allows scrapers to focus on dynamic data (teams, rosters, matches)
- Clean separation: structure (SQL) vs content (scrapers)

**CSL Example**:
- CSL has consistent divisions: Division 1, Division 1 Reserve, Division 2, Division 2 Reserve, Division 3, Division 4, Over-40 Division, Spring Division
- These divisions repeat across seasons (2022/2023, 2023/2024, 2024/2025, etc.)
- Scraper should only create teams and team assignments, not divisions

### 3. Dynamic Scraped Data (Database Only)
**Location**: Database only (NOT in SQL files)  
**When Loaded**: Via `update.sh` (scraper runs)  
**Source**: External websites (APSL, CSL, CASA, GroupMe)  
**Persistence**: Survives rebuilds via Docker volumes

**Data Types**:
- **Teams**: Team rosters change each season
- **Players**: Player data (names, jerseys, etc.)
- **Matches**: Match schedules and results
- **Match Events**: Goals, substitutions, cards
- **Standings**: Calculated from match results
- **Team Assignments**: Which teams play in which divisions (division_teams table)

**Why NOT in SQL Files**:
- Changes frequently (weekly for matches/events)
- Large volume (1000+ players, 250+ matches)
- External source of truth (league websites)
- Would create massive git diffs on every scrape

**Scraper Behavior**:
- Upserts data (insert new, update existing)
- Uses external_id + source_system_id for deduplication
- Respects scrape_status_id state machine (not_started ‚Üí in_progress ‚Üí completed)

### 4. One-Time Scraped Data (SQL Files Generated by Scrapers)
**Location**: `database/data/*.sql` (generated, then committed)  
**When Loaded**: Every rebuild  
**Source**: Scraped once, then maintained manually

**Use Cases**:
- **Venues**: Scraped once from Google Places API, then maintained
  - File: `036-venues.sql` (to be created)
  - Command: Run venue scraper once, commit output, set scrape_action to 'skip'
  
- **Historical Reference Data**: One-time imports from external sources
  - Example: Initial league/season/division setup from league websites

**Workflow**:
1. Write scraper to generate SQL INSERT statements
2. Run scraper once: `./update.sh`
3. Copy generated SQL to `database/data/0XX-name.sql`
4. Commit file to git
5. Set scrape target to `scrape_action_id = 2` (skip) or `is_active = false`
6. Future rebuilds load from committed SQL file

## Current State Analysis

### ‚úÖ What's Currently in SQL Files
- Schema and lookup tables
- Scrape target configurations
- User/admin/person records (manual)
- Continents reference data

### ‚ùå What's Missing from SQL Files (Should Be Added)
- **Organizations** (APSL, CSL, CASA) - currently scraped
- **Leagues** (APSL, CSL, CASA) - currently scraped
- **Seasons** (2022/2023, 2023/2024, 2024/2025, etc.) - currently scraped
- **Conferences** (APSL: Metropolitan, National, etc.) - currently scraped
- **Divisions** (CSL: Division 1, Division 2, etc.) - currently scraped
- **Clubs** (Lighthouse 1893 SC, etc.) - currently scraped
- **Venues** (should be one-time scrape ‚Üí SQL file)

### üîÑ What's Correctly Dynamic
- Teams (change each season)
- Players (rosters change)
- Matches (scheduled weekly)
- Match events (goals, subs, cards)
- Standings (derived from matches)

## Migration Plan

### Phase 1: Extract League Structure to SQL Files
Create these files:
1. `database/data/030-organizations.sql` - APSL, CSL, CASA
2. `database/data/031-leagues.sql` - League definitions
3. `database/data/032-seasons.sql` - Known seasons (last 3 years + current)
4. `database/data/033-conferences.sql` - APSL conferences
5. `database/data/034-divisions.sql` - CSL divisions (all seasons)
6. `database/data/035-clubs.sql` - Manually managed clubs

### Phase 2: Update Scrapers
Modify scrapers to:
- **NOT** create organizations/leagues/seasons/conferences/divisions
- **ONLY** create teams and team assignments (division_teams)
- Look up existing structure by name/season
- Log warnings if structure not found

### Phase 3: Venues One-Time Scrape
1. Run venue scraper to generate SQL
2. Commit `036-venues.sql`
3. Disable venue scraper

## Scraper Configuration

### Scrape Actions
Set `scrape_action_id` in `scrape_targets` table:
- `1` (download_and_parse) - Active scraping
- `2` (skip) - Don't run (data in SQL files)
- `3` (download_only) - Cache HTML only
- `4` (parse_only) - Parse cached HTML

### Scrape Status State Machine
- `1` (not_started) - Ready to run
- `2` (in_progress) - Currently running
- `3` (completed) - Successfully completed
- `4` (needs_refresh) - Needs re-scraping
- `5` (failed) - Error occurred
- `6` (archived) - Old/deprecated target

### Example: Disable a Scraper After One-Time Run
```sql
-- Mark venue scraper as skip (data now in SQL file)
UPDATE scrape_targets 
SET scrape_action_id = 2  -- skip
WHERE scraper_type_id = 6 AND label LIKE '%Venue%';
```

## Best Practices

### When to Use SQL Files
‚úÖ Data is known in advance (league structures)  
‚úÖ Data rarely changes (organizations, divisions)  
‚úÖ Data created manually (users, admins)  
‚úÖ One-time imports (venues, historical data)  
‚úÖ Reference/lookup tables (positions, match types)

### When to Use Scrapers
‚úÖ Data changes frequently (matches, standings)  
‚úÖ Large volume of data (players, match events)  
‚úÖ External source of truth (league websites)  
‚úÖ Data updates weekly/daily (rosters, schedules)

### When to Use One-Time Scrape ‚Üí SQL
‚úÖ Data doesn't change often (venues)  
‚úÖ Need reproducible builds (same venues every time)  
‚úÖ Want to version control data (git commits)  
‚úÖ External API has rate limits (Google Places)

## File Naming Convention

SQL files in `database/data/` are loaded **alphabetically**:
- `000-099`: Schema and core lookups
- `013-019`: Scrape system tables
- `020-029`: User/person/admin data
- `030-039`: League structure (organizations, leagues, seasons, divisions)
- `040-049`: Clubs and teams (manual/static)
- `050-089`: Application features (standings, events, etc.)
- `090-099`: Views and functions

Use letter suffixes (a/b/c) when order matters within same number:
- `032a-apsl-seasons.sql`
- `032b-csl-seasons.sql`
- `032c-casa-seasons.sql`

## Verification Commands

```bash
# Check what's loaded in database
podman exec footballhome_db psql -U footballhome_user -d footballhome -c \
  "SELECT 'organizations', COUNT(*) FROM organizations UNION ALL 
   SELECT 'leagues', COUNT(*) FROM leagues UNION ALL 
   SELECT 'seasons', COUNT(*) FROM seasons UNION ALL 
   SELECT 'divisions', COUNT(*) FROM divisions;"

# Check scrape target status
podman exec footballhome_db psql -U footballhome_user -d footballhome -c \
  "SELECT id, label, sa.name as action, ss.name as status 
   FROM scrape_targets st 
   LEFT JOIN scrape_actions sa ON st.scrape_action_id = sa.id 
   LEFT JOIN scrape_statuses ss ON st.scrape_status_id = ss.id 
   ORDER BY id;"

# Test full rebuild (destroys data, recreates from SQL files)
./build.sh
```

## Summary

**Current Issue**: League structure (organizations, leagues, seasons, divisions) is being scraped repeatedly, causing duplicates and rebuild inconsistencies.

**Solution**: Move league structure to SQL files, scrapers only create dynamic content (teams, players, matches).

**Benefit**: 
- No more duplicate divisions
- Reproducible builds
- Clear data ownership
- Scrapers simpler and faster
- Easy to add new seasons/divisions manually
